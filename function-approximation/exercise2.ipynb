{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Q value by 3 samples, using ϵ-greedy Q with ϵ=0.25, α=0.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:\n",
      "State: C, Action: 0, Reward: -1, Weights: [-3.8 -0.6 -2.6]\n",
      "\n",
      "Step 2:\n",
      "State: D, Action: 1, Reward: -1, Weights: [-11.48   3.24  -6.44]\n",
      "\n",
      "Step 3:\n",
      "State: C, Action: 0, Reward: -1, Weights: [13.672 11.624  1.944]\n",
      "\n",
      "Final weights after 3 samples: [13.672 11.624  1.944]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "epsilon = 0.25\n",
    "alpha = 0.8\n",
    "gamma = 1.0\n",
    "num_steps = 3\n",
    "states = ['A', 'B', 'C', 'D', 'E']\n",
    "terminal_states = ['A', 'E']\n",
    "actions = [0, 1]  # 0: Left, 1: Right\n",
    "\n",
    "# Rewards\n",
    "rewards = {\n",
    "    'A': 1000,\n",
    "    'E': 10,\n",
    "    'B': -1,\n",
    "    'C': -1,\n",
    "    'D': -1\n",
    "}\n",
    "\n",
    "# Initialize Q(S, A) using weights w = [1, 1, -1] for linear approximation\n",
    "weights = np.array([1.0, 1.0, -1.0])  # Initial weights for [distance to left wall, turn direction, bias]\n",
    "\n",
    "# Feature extraction function\n",
    "def feature_vector(state, action):\n",
    "    # Distance to the left wall\n",
    "    left_wall_dist = states.index(state)\n",
    "    \n",
    "    # Turn direction: +1 for left (action=0), -1 for right (action=1)\n",
    "    turn_dir = 1 if action == 0 else -1\n",
    "    \n",
    "    # Bias term\n",
    "    bias = 1\n",
    "    \n",
    "    # Feature vector: [distance, turn direction, bias]\n",
    "    return np.array([left_wall_dist, turn_dir, bias])\n",
    "\n",
    "# Calculate Q-value using linear function approximation\n",
    "def q_value(state, action):\n",
    "    x_sa = feature_vector(state, action)\n",
    "    return np.dot(weights, x_sa)\n",
    "\n",
    "# epsilon-greedy policy\n",
    "def epsilon_greedy(state):\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(actions)  # Explore (random action)\n",
    "    else:\n",
    "        # Exploit: select action with the highest Q-value\n",
    "        q_vals = [q_value(state, a) for a in actions]\n",
    "        return np.argmax(q_vals)\n",
    "\n",
    "# Transition function (deterministic)\n",
    "def transition(state, action):\n",
    "    if state == 'D':\n",
    "        return 'C' if action == 0 else 'E'\n",
    "    elif state == 'C':\n",
    "        return 'B' if action == 0 else 'D'\n",
    "    elif state == 'B':\n",
    "        return 'A' if action == 0 else 'C'\n",
    "    return state  # A and E are terminal, no transition\n",
    "\n",
    "# Main Q-learning with 3 samples\n",
    "state = 'D'  # Start at initial state D\n",
    "\n",
    "for _ in range(num_steps):\n",
    "    # Select action using epsilon-greedy policy\n",
    "    action = epsilon_greedy(state)\n",
    "    \n",
    "    # Get the next state and reward\n",
    "    next_state = transition(state, action)\n",
    "    reward = rewards[next_state]\n",
    "    \n",
    "    # Calculate TD target\n",
    "    if next_state in terminal_states:\n",
    "        td_target = reward\n",
    "    else:\n",
    "        max_next_q = max([q_value(next_state, a) for a in actions])  # Best Q-value for next state\n",
    "        td_target = reward + gamma * max_next_q\n",
    "    \n",
    "    # Update Q-value using linear approximation (update weights)\n",
    "    q_current = q_value(state, action)\n",
    "    td_error = td_target - q_current\n",
    "    weights += alpha * td_error * feature_vector(state, action)\n",
    "    \n",
    "    # Move to the next state\n",
    "    state = next_state\n",
    "    \n",
    "    # Output current Q-values\n",
    "    print(f\"Step {_ + 1}:\")\n",
    "    print(f\"State: {state}, Action: {action}, Reward: {reward}, Weights: {weights}\\n\")\n",
    "\n",
    "# Final Q-values\n",
    "print(\"Final weights after 3 samples:\", weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the optimal weight value w look like? (Hint: you can train until convergence, or give some explanation of the optimal weight value w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 56 iterations.\n",
      "Optimal weight vector: [815.04136858  24.04616771 975.95383177]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "epsilon = 0.25\n",
    "alpha = 0.8\n",
    "gamma = 1.0\n",
    "convergence_threshold = 1e-6  # Stop if weight changes are smaller than this\n",
    "max_iterations = 1000  # Maximum iterations for training\n",
    "states = ['A', 'B', 'C', 'D', 'E']\n",
    "terminal_states = ['A', 'E']\n",
    "actions = [0, 1]  # 0: Left, 1: Right\n",
    "\n",
    "# Rewards\n",
    "rewards = {\n",
    "    'A': 1000,\n",
    "    'E': 10,\n",
    "    'B': -1,\n",
    "    'C': -1,\n",
    "    'D': -1\n",
    "}\n",
    "\n",
    "# Initialize Q(S, A) using weights w = [1.0, 1.0, -1.0] for linear approximation\n",
    "weights = np.array([1.0, 1.0, -1.0])\n",
    "\n",
    "# Feature extraction function\n",
    "def feature_vector(state, action):\n",
    "    # Distance to the left wall\n",
    "    left_wall_dist = states.index(state)\n",
    "    \n",
    "    # Turn direction: +1 for left (action=0), -1 for right (action=1)\n",
    "    turn_dir = 1 if action == 0 else -1\n",
    "    \n",
    "    # Bias term\n",
    "    bias = 1\n",
    "    \n",
    "    # Feature vector: [distance, turn direction, bias]\n",
    "    return np.array([left_wall_dist, turn_dir, bias])\n",
    "\n",
    "# Calculate Q-value using linear function approximation\n",
    "def q_value(state, action):\n",
    "    x_sa = feature_vector(state, action)\n",
    "    return np.dot(weights, x_sa)\n",
    "\n",
    "# epsilon-greedy policy\n",
    "def epsilon_greedy(state):\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(actions)  # Explore (random action)\n",
    "    else:\n",
    "        # Exploit: select action with the highest Q-value\n",
    "        q_vals = [q_value(state, a) for a in actions]\n",
    "        return np.argmax(q_vals)\n",
    "\n",
    "# Transition function (deterministic)\n",
    "def transition(state, action):\n",
    "    if state == 'D':\n",
    "        return 'C' if action == 0 else 'E'\n",
    "    elif state == 'C':\n",
    "        return 'B' if action == 0 else 'D'\n",
    "    elif state == 'B':\n",
    "        return 'A' if action == 0 else 'C'\n",
    "    return state  # A and E are terminal, no transition\n",
    "\n",
    "# Train Q-learning until convergence\n",
    "def train_until_convergence():\n",
    "    global weights\n",
    "    state = 'D'  # Start at initial state D\n",
    "    for iteration in range(max_iterations):\n",
    "        # Store old weights for comparison\n",
    "        old_weights = weights.copy()\n",
    "        \n",
    "        # Select action using epsilon-greedy policy\n",
    "        action = epsilon_greedy(state)\n",
    "        \n",
    "        # Get the next state and reward\n",
    "        next_state = transition(state, action)\n",
    "        reward = rewards[next_state]\n",
    "        \n",
    "        # Calculate TD target\n",
    "        if next_state in terminal_states:\n",
    "            td_target = reward\n",
    "        else:\n",
    "            max_next_q = max([q_value(next_state, a) for a in actions])  # Best Q-value for next state\n",
    "            td_target = reward + gamma * max_next_q\n",
    "        \n",
    "        # Update Q-value using linear approximation (update weights)\n",
    "        q_current = q_value(state, action)\n",
    "        td_error = td_target - q_current\n",
    "        weights += alpha * td_error * feature_vector(state, action)  # Update weights\n",
    "        \n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check for convergence (if weight changes are smaller than threshold)\n",
    "        if np.linalg.norm(weights - old_weights) < convergence_threshold:\n",
    "            print(f\"Converged after {iteration + 1} iterations.\")\n",
    "            break\n",
    "\n",
    "train_until_convergence()\n",
    "\n",
    "# Output final weights\n",
    "print(\"Optimal weight vector:\", weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Represent the value function by two vectors, w1 for turn left, w2 for turn right, what is the optimal weight value in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 943 iterations.\n",
      "Optimal weight vector for Left (w1): [ 10.892976   500.99996537 498.99996537]\n",
      "Optimal weight vector for Right (w2): [   1.         -459.90435654  459.90435654]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "epsilon = 0.25\n",
    "alpha = 0.01\n",
    "gamma = 1.0\n",
    "convergence_threshold = 1e-6  # Stop if weight changes are smaller than this\n",
    "max_iterations = 1000  # Maximum iterations for training\n",
    "states = ['A', 'B', 'C', 'D', 'E']\n",
    "terminal_states = ['A', 'E']\n",
    "actions = [0, 1]  # 0: Left, 1: Right\n",
    "\n",
    "# Rewards\n",
    "rewards = {\n",
    "    'A': 1000,\n",
    "    'E': 10,\n",
    "    'B': -1,\n",
    "    'C': -1,\n",
    "    'D': -1\n",
    "}\n",
    "\n",
    "# Initialize Q(S, A) with separate weight vectors w1 for Left and w2 for Right\n",
    "w1 = np.array([1.0, 1.0, -1.0])  # Weights for left action (0)\n",
    "w2 = np.array([1.0, 1.0, -1.0])  # Weights for right action (1)\n",
    "\n",
    "# Feature extraction function\n",
    "def feature_vector(state, action):\n",
    "    # Distance to the left wall\n",
    "    left_wall_dist = states.index(state)\n",
    "    \n",
    "    # Turn direction: +1 for left (action=0), -1 for right (action=1)\n",
    "    turn_dir = 1 if action == 0 else -1\n",
    "    \n",
    "    # Bias term\n",
    "    bias = 1\n",
    "    \n",
    "    # Feature vector: [distance, turn direction, bias]\n",
    "    return np.array([left_wall_dist, turn_dir, bias])\n",
    "\n",
    "# Calculate Q-value using linear function approximation for both actions\n",
    "def q_value(state, action):\n",
    "    x_sa = feature_vector(state, action)\n",
    "    if action == 0:\n",
    "        return np.dot(w1, x_sa)\n",
    "    else:\n",
    "        return np.dot(w2, x_sa)\n",
    "\n",
    "# epsilon-greedy policy\n",
    "def epsilon_greedy(state):\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(actions)  # Explore (random action)\n",
    "    else:\n",
    "        # Exploit: select action with the highest Q-value\n",
    "        q_vals = [q_value(state, a) for a in actions]\n",
    "        return np.argmax(q_vals)\n",
    "\n",
    "# Transition function (deterministic)\n",
    "def transition(state, action):\n",
    "    if state == 'D':\n",
    "        return 'C' if action == 0 else 'E'\n",
    "    elif state == 'C':\n",
    "        return 'B' if action == 0 else 'D'\n",
    "    elif state == 'B':\n",
    "        return 'A' if action == 0 else 'C'\n",
    "    return state  # A and E are terminal, no transition\n",
    "\n",
    "# Train Q-learning until convergence\n",
    "def train_until_convergence():\n",
    "    global w1, w2\n",
    "    state = 'D'  # Start at initial state D\n",
    "    for iteration in range(max_iterations):\n",
    "        # Store old weights for comparison\n",
    "        w1_old = w1.copy()\n",
    "        w2_old = w2.copy()\n",
    "        \n",
    "        # Select action using epsilon-greedy policy\n",
    "        action = epsilon_greedy(state)\n",
    "        \n",
    "        # Get the next state and reward\n",
    "        next_state = transition(state, action)\n",
    "        reward = rewards[next_state]\n",
    "        \n",
    "        # Calculate TD target\n",
    "        if next_state in terminal_states:\n",
    "            td_target = reward\n",
    "        else:\n",
    "            max_next_q = max([q_value(next_state, a) for a in actions])  # Best Q-value for next state\n",
    "            td_target = reward + gamma * max_next_q\n",
    "        \n",
    "        # Update Q-value using linear approximation (update weights)\n",
    "        q_current = q_value(state, action)\n",
    "        td_error = td_target - q_current\n",
    "        \n",
    "        if action == 0:\n",
    "            w1 += alpha * td_error * feature_vector(state, action)  # Update w1 for left action\n",
    "        else:\n",
    "            w2 += alpha * td_error * feature_vector(state, action)  # Update w2 for right action\n",
    "        \n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check for convergence (if weights change is smaller than threshold)\n",
    "        if np.linalg.norm(w1 - w1_old) < convergence_threshold and np.linalg.norm(w2 - w2_old) < convergence_threshold:\n",
    "            print(f\"Converged after {iteration + 1} iterations.\")\n",
    "            break\n",
    "\n",
    "\n",
    "train_until_convergence()\n",
    "\n",
    "# Output final weights for w1 and w2\n",
    "print(\"Optimal weight vector for Left (w1):\", w1)\n",
    "print(\"Optimal weight vector for Right (w2):\", w2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
